{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experimental Test Code:\n",
    "- Code example for testing Anomaly Detection algorithms on Smart-Manufacturing datasets.\n",
    "- Please refer to [ADBench](https://github.com/Minqi824/ADBench) package to use of additional algorithms or not manufacturing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:41:53.643140Z",
     "start_time": "2022-07-08T07:41:41.552946Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import basic package\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import the necessary package\n",
    "from data_generator import DataGenerator\n",
    "from myutils import Utils\n",
    "\n",
    "# instiantiate datagenerator and util objects\n",
    "datagenerator = DataGenerator() \n",
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We include all the datasets for Smart-Manufacturing in the \"datasets\" folder, as the \"number_data.npz\" filename. Please see the table in the markdown for details. You can specify the dataset name by removing the filename \".npz\" suffix in the data generator, e.g., \"88_GenesisPickPlace.npz\" as \"88_GenesisPickPlace\". \n",
    "    \n",
    "    \n",
    "- All the algorithms included are explained in detail in the [ADBench](https://github.com/Minqi824/ADBench) resource.\n",
    "    - You need to specify the model name when initialization, as some algorithms (e.g., supervised algorithms) are integrated in one class, please see [ADBench](https://github.com/Minqi824/ADBench) for details.\n",
    "    - You can also test your own AD algorithms on the proposed datasets, as long as the algorithm can output anomaly score for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:41:53.675055Z",
     "start_time": "2022-07-08T07:41:53.648127Z"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir('datasets/Classical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:41:55.627834Z",
     "start_time": "2022-07-08T07:41:53.682035Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import AD algorimths or class of algorithms to be used\n",
    "from baseline.PyOD import PYOD\n",
    "from baseline.DevNet.run import DevNet\n",
    "from baseline.Supervised import supervised\n",
    "from baseline.LSTMOD.LSTMOD import LSTMOutlierDetector\n",
    "from baseline.GANomaly.run import GANomaly\n",
    "from pyod.models.vae import VAE\n",
    "\n",
    "# dataset and model list / dict\n",
    "dataset_list = ['133_HighStorageSystem_anreal', '135_GenesisPickPlace_anreal'] # Add the datasets you want to test.\n",
    "\n",
    "model_dict = {'CBLOF':PYOD,'OCSVM':PYOD,'HBOS':PYOD,'KNN':PYOD,'LOF':PYOD,'PCA':PYOD,'IForest': PYOD, # Classical\n",
    "              'DeepSVDD': PYOD,'AutoEncoder': PYOD,'VAE':PYOD,  # Deep\n",
    "              'LSTMOutlierDetector': LSTMOutlierDetector,'DevNet': DevNet,'GANomaly': GANomaly, # Deep\n",
    "              'XGBOD':PYOD,'RF': supervised, 'CatB': supervised} # Supervised\n",
    "\n",
    "# dataframes to save the results\n",
    "df_AUCROC = pd.DataFrame(data=None, index=dataset_list, columns = model_dict.keys())\n",
    "df_AUCPR = pd.DataFrame(data=None, index=dataset_list, columns = model_dict.keys())\n",
    "df_TIMETRAIN = pd.DataFrame(data=None, index=dataset_list, columns = model_dict.keys())\n",
    "df_TIMEINFER = pd.DataFrame(data=None, index=dataset_list, columns = model_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for reproducible results\n",
    "seed = 42\n",
    "\n",
    "for dataset in dataset_list:\n",
    "    '''\n",
    "    la: ratio of labeled anomalies, from 0.0 to 1.0\n",
    "    realistic_synthetic_mode: types of synthetic anomalies, can be local, global, dependency or cluster\n",
    "    noise_type: inject data noises for testing model robustness, can be duplicated_anomalies, irrelevant_features or label_contamination\n",
    "    '''\n",
    "    \n",
    "    # import the dataset\n",
    "    datagenerator.dataset = dataset # specify the dataset name\n",
    "    data = datagenerator.generator(la=0.1, realistic_synthetic_mode=None, noise_type=None) \n",
    "    for name, clf in model_dict.items():\n",
    "        # model initialization.\n",
    "        # You can make special cases of AD algorithms (in this case, VAE) to tune hyperparameters:\n",
    "        if name == 'VAE':\n",
    "            # model initialization\n",
    "            clf = VAE(encoder_neurons =[64, 32, 1],decoder_neurons =[1, 32, 64])\n",
    "\n",
    "            # training, for unsupervised models the y label will be discarded\n",
    "            try:\n",
    "                start_train = time.time()\n",
    "                clf = clf.fit(data['X_train'], data['y_train'])\n",
    "                duracion_train = (time.time() - start_train)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # output predicted anomaly score on testing set\n",
    "            try:\n",
    "                start_infer = time.time()\n",
    "                score = clf.decision_function(pd.DataFrame(data['X_test']))\n",
    "                duracion_infer = (time.time() - start_infer)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        else:\n",
    "            # model initialization\n",
    "            clf = clf(seed=seed, model_name=name)\n",
    "            \n",
    "            # training, for unsupervised models the y label will be discarded\n",
    "            try:\n",
    "                start_train = time.time()\n",
    "                clf = clf.fit(data['X_train'], data['y_train'])\n",
    "                duracion_train = (time.time() - start_train)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # output predicted anomaly score on testing set\n",
    "            try:\n",
    "                start_infer = time.time()\n",
    "                score = clf.predict_score(data['X_test'])\n",
    "                duracion_infer = (time.time() - start_infer)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # evaluation\n",
    "        try:\n",
    "            result = utils.metric(y_true=data['y_test'], y_score=score)\n",
    "        except:\n",
    "            result = {'aucroc':np.float('nan'),'aucpr':np.float('nan')}\n",
    "            pass\n",
    "        \n",
    "        # save results\n",
    "        df_AUCROC.loc[dataset, name] = result['aucroc']\n",
    "        df_AUCPR.loc[dataset, name] = result['aucpr']\n",
    "        df_TIMETRAIN.loc[dataset, name] = duracion_train\n",
    "        try:\n",
    "            df_TIMEINFER.loc[dataset, name] = duracion_infer\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:50:14.555117Z",
     "start_time": "2022-07-08T07:50:14.511234Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_AUCROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-08T07:50:14.587032Z",
     "start_time": "2022-07-08T07:50:14.560106Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_AUCPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TIMETRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TIMEINFER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
